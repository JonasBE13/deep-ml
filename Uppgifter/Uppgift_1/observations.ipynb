{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Questions\n",
    "After completing the tasks, answer the following questions:\n",
    "\n",
    "1. How did different activation functions affect the model's performance?\n",
    "2. How did different loss functions impact the learning process and accuracy?\n",
    "3. Which combination of activation and loss function provided the best results for this task?\n",
    "\n",
    "* 1. Det är inte speciellt stor skillnad mellan dem olika modellerna i själva resultatet. Skillnaden man ser är hur många epocher det tar för dem olika modellerna att nå \"diminishing returns\". Vissa går upp och lägger sig stabilt efter bara några få epocher, så som relu och sigmoid. Tanh tar en lite mer \"hackig\" väg fram till reslutatet, som den stegar genom en trappa. Det tar fler epocher för att den ska nå ungefär samma resultat som den andra modellerna, men kontentan blir tillslut detsamma ändå. Accuracy och loss är väldigt lika bland relu, sigmoid och tanh, åtminstone när man kör med binary_crossentropy som loss function. Den största skillnaden på resultatet ser man när man istället kör med mean_squared_error som loss function.\n",
    "\n",
    "        Det är spännande att se hur sigmoid-modellen peakar väldigt tidigt, för att sedan gå ner lite, och till slut upp igen för att lägga sig stabilt.\n",
    "\n",
    "* 2. med mean_squared_error som loss function på relu så ser vi att det tar betydligt fler epocher för relu-modellen att nå \"diminishing returns\", men resultatet blir bättre. med binary_crossentropy så når den sitt maximum redan efter ~30 epocher, medan den fortfarande ser ut att stiga efter 100 epocher med mean_squared_error. För tanh och sigmoid så ser modellerna ganska lika ut varandra, oavsett loss function. det som skiljer dem är att \"test loss\" är mycket lägre när man kör mean_squared_error.\n",
    "    \n",
    "\n",
    "* 3. Bästa resultatet ser vi med relu-modellen med mean_squared_error loss function. Man ser en generell förbättring av mean_squared_error som loss function på alla modeller faktiskt, men relu-modellen presterade bäst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# relu observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu with binary_crossentropy\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu with mean_squared_error\n",
    "\n",
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# sigmoid observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid with binary_crossentropy\n",
    "\n",
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid with mean_squared_error\n",
    "\n",
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# __tanh__ observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh with binary_crossentropy\n",
    "\n",
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh with mean_squared_error\n",
    "\n",
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
